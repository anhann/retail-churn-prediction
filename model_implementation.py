# -*- coding: utf-8 -*-
"""Model implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vbTLut2XD7PbZGijyPDsa6BSSas4cebA
"""

# RUNNING ONLY
# import library
import psycopg2
import numpy as np
import pandas as pd
from getpass import getpass

# REQUIRE INPUT
# fill in username/ password
user = 'lixan23'
db_ip = '10.158.72.112'
################################
pw = getpass('Enter the database password for {}: '.format(user))

# Commented out IPython magic to ensure Python compatibility.
# RUNNING ONLY
# run this train set creation function - still include all feature if later require statistics
# redundant features will be filter in data process function below
# dont drop customer_id this time
def get_dataset( reference_day, tumbling_window_size = 38, output_window_size = 38 ):
    with psycopg2.connect("host='{}' dbname='nlab' user='{}' password='{}'".format(db_ip, user, pw)) as conn:
      sql = """
      WITH 
      tumbling AS(
      SELECT customer_id,
#              %(ref_date)s::date AS ref_day,
             COUNT (DISTINCT CASE WHEN purchased_at > %(ref_date)s::DATE AND purchased_at <= %(ref_date)s::DATE + %(ows)s THEN purchased_at::DATE ELSE null END) as output_feature,
             SUM(CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s AND purchased_at <= %(ref_date)s::DATE THEN value ELSE 0 END) as sale_f1,
             SUM(CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s*2 AND purchased_at <= %(ref_date)s::DATE-%(ws)s THEN value ELSE 0 END ) as sale_f2,
             SUM(CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s*3 AND purchased_at <= %(ref_date)s::DATE-%(ws)s*2 THEN value ELSE 0 END ) as sale_f3,
             SUM(CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s*4 AND purchased_at <= %(ref_date)s::DATE-%(ws)s*3 THEN value ELSE 0 END ) as sale_f4,
             SUM(CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s*5 AND purchased_at <= %(ref_date)s::DATE-%(ws)s*4 THEN value ELSE 0 END ) as sale_f5,
             COUNT (DISTINCT CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s AND purchased_at <= %(ref_date)s::DATE THEN purchased_at::DATE ELSE null END ) as visit_f1,
             COUNT (DISTINCT CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s*2 AND purchased_at <= %(ref_date)s::DATE-%(ws)s THEN purchased_at::DATE ELSE null END ) as visit_f2,
             COUNT (DISTINCT CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s*3 AND purchased_at <= %(ref_date)s::DATE-%(ws)s*2 THEN purchased_at::DATE ELSE null END ) as visit_f3,
             COUNT (DISTINCT CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s*4 AND purchased_at <= %(ref_date)s::DATE-%(ws)s*3 THEN purchased_at::DATE ELSE null END ) as visit_f4,
             COUNT (DISTINCT CASE WHEN purchased_at > %(ref_date)s::DATE -%(ws)s*5 AND purchased_at <= %(ref_date)s::DATE-%(ws)s*4 THEN purchased_at::DATE ELSE null END ) as visit_f5
     FROM ml2.receipts_clean
     JOIN ml2.receipt_lines_clean
     USING (receipt_id)
     GROUP BY customer_id),
     
     rfm AS (
     SELECT customer_id,
             SUM(value) as total_spend,
             COUNT (DISTINCT purchased_at::DATE) as active_day,
             SUM(value)/COUNT (DISTINCT purchased_at) as basket_value,
             SUM (value)/ SUM (qty) as unit_cost,
             COUNT (DISTINCT product_code) as num_pro,
             COUNT(DISTINCT purchased_at::DATE)/((EXTRACT(day from (MAX(purchased_at)-MIN(purchased_at)))/%(ws)s)::INT+1) AS average_visit,
             SUM (value)/((EXTRACT(day from (MAX(purchased_at)-MIN(purchased_at)))/%(ws)s)::INT+1) AS average_spend
     FROM ml2.receipts_clean
     JOIN ml2.receipt_lines_clean
     USING (receipt_id)
     --below condition to make sure we do not include future data--
     WHERE purchased_at<=%(ref_date)s::date
     --only consider the nearest 5 period
     AND purchased_at>%(ref_date)s::DATE -%(ws)s*5
     GROUP BY customer_id
     HAVING MAX(purchased_at)>%(ref_date)s::DATE-%(ws)s),
             
     --create static value as most frequently visit store--
     store AS (
     SELECT customer_id, store_code 
            FROM (SELECT customer_id, store_code, COUNT(*) AS ct
                    FROM ml2.receipts_clean
                    GROUP BY 1,2)x
            GROUP BY 1,2,ct
            HAVING ct=MAX(ct))
    
    SELECT * FROM tumbling JOIN store USING (customer_id) JOIN rfm USING (customer_id)
    
      """

      df = pd.read_sql(sql, conn, params = {'ref_date':reference_day, 'ws':tumbling_window_size, 'ows':output_window_size})

      return df.drop(columns = ['ref_day','output_feature'], inplace = False), df.output_feature

# RUNNING ONLY
# create processing function includes:
def data_process (X,y):
    for i in range(len(y)):
        # process output to churn/ non churn based on decreasethreshold = 0.49
        if y[i]<=X.iloc[i,-2]*0.49:
            y[i]=1
        else:
            y[i]=0
    # one hot coding process for store inputs
    from sklearn.preprocessing import LabelBinarizer
    lb = LabelBinarizer()
    def binary_convert(df,a):
        lb_results = lb.fit_transform(df[a])
        df_onehot = pd.DataFrame(lb_results, columns=lb.classes_)
        for col in list(df_onehot.columns):
            df_onehot.rename(columns={col:(a+'_'+str(col))},inplace = True)
        df=df.drop(columns=a)
        new_df=pd.concat([df,df_onehot],axis=1)
        return new_df
    X_new=binary_convert(X,'store_code')
    # define X for prediction
    X_min=X_new[['visit_f1','visit_f2','visit_f3','visit_f4','visit_f5','active_day','basket_value','unit_cost','num_pro','average_visit']]
    return X_new, X_min, y

# REQUIRE INPUT
import datetime  
from datetime import timedelta
import numpy as np
# please input the latest date here - for now running the lastest date from database
today= datetime.date(2020,11,24)

# RUNNING ONLY
# getting data
# training ref day will be today - 76 days
ref_train= str(today-timedelta(days=38))
X_train, y_train  = get_dataset(ref_train)
X_train, X_train_min, y_train=data_process (X_train,y_train)
# test ref day will be today - 38 days
# y_test will receive 0 value as we're at observation period
ref_test= str(today)
X_test, y_test  = get_dataset(ref_test)
X_test, X_test_min, y_test=data_process (X_test,y_test)

# RUNNING ONLY
# fitting model
from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier(random_state=42, max_depth=3, n_estimators=50)
rf.fit(X_train_min,y_train)
# merging data with prediction
predict_percent=pd.DataFrame(rf.predict_proba(X_test_min))[1]
predict_label=pd.DataFrame(rf.predict(X_test_min))
df=pd.concat([X_test,predict_percent,predict_label],axis=1)
# Rename the columns
df.rename(columns={1:'churn_percent'},inplace=True)
df.rename(columns={0:'churn_label'},inplace=True)
# Rename the prediction result to churn - non churn
for i in range(len(df)):
    if df['churn_label'][i]==1:
        df['churn_label'][i]='Churn'
    else:
        df['churn_label'][i]='Non_churn'

# RUNNING ONLY
# Create ranking columns based on prediction probability
# 1: >=90# ... 10:<10%
df['ranking']=((1-df['churn_percent'])*100)//10+1

# RUNNING ONLY
# generate list of customer to be targeted
# list contain people to be targeted at top
df[['customer_id','total_spend','churn_percent','ranking']].sort_values(by='ranking')

# RUNNING ONLY
# Calculating % of each group - choosing relevant group to target based on their % and active day
# People with 1 active day - First time group
# People with >=2 - Rising interest group
for c, d in df.groupby('ranking'):
    m=np.mean(d['active_day'])
    print ('- The target group {} with churn probability around {}% contains {} people, \naccounts for {}% of active base, has {} active days'.format(c, 100-(c+1)*10,len(d),round(len(d)*100/len(df),1),round(m,1)))

# RUNNING ONLY
# Full data table by group
pd.set_option('display.max_columns', None)
for c, d in df.groupby('ranking'):
    print (c)
    print('% size ',len(d)/len(df)*100)
    print ('% sale: ',np.sum(d['total_spend'])/np.sum(df['total_spend'])*100)
    display(d.describe())

# REQUIRE INPUT
# create list of offer taker people to monitor
offer=[0]*len(df)
df['offer']=offer
for i in range(len(df)):
    if df['ranking'][i]<=2: #please input target group, assume currently target 1,2
        df['offer'][i]+=1
df[['customer_id','total_spend','churn_percent','ranking','offer']]

